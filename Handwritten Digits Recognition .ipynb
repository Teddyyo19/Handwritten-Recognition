{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2>What is Deep Learning?</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief Theory: Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Practice, defining the term \"Deep\": in this context, deep means that we are studying a Neural Network which has several hidden layers (more than one), no matter what type (convolutional, pooling, normalization, fully-connected etc). The most interesting part is that some papers noticed that Deep Neural Networks with the right architectures/hyper-parameters achieve better results than shallow Neural Networks with the same computational power (e.g. number of neurons or connections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Practice, defining \"Learning\": In the context of supervised learning, digits recognition in our case, the learning part consists of a target/feature which is to be predicted using a given set of observations with the already known final prediction (label). In our case, the target will be the digit (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) and the observations are the intensity and relative position of the pixels. After some training, it is possible to generate a \"function\" that map inputs (digit image) to desired outputs(type of digit). The only problem is how well this map operation occurs. While trying to generate this \"function\", the training process continues until the model achieves a desired level of accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first classify MNIST using a simple Multi-layer perceptron and then, in the second part, we use deep learning to improve the accuracy of our results.\n",
    "<a id=\"ref3\"></a>\n",
    "<h2>1st part: Classify MNIST using a simple model.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a simple Multi-layer perceptron, a simple type of Neural Network, to perform classification tasks on the MNIST digits dataset. If you are not familiar with the MNIST dataset, please consider to read more about it: click here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is MNIST?\n",
    "According to LeCun's website, the MNIST is a: \"database of handwritten digits that has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the MNIST dataset using TensorFlow built-in feature</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's very important to notice that MNIST is a high optimized data-set and it does not contain images. You will need to build your own code if you want to see the real digits. Another important side note is the effort that the authors invested on this data-set with normalization and centering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN to classify handwritten digits from MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot \n",
    "import numpy as np\n",
    "# keras imports for the dataset and building our neural network\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating placeholders<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input and output \n",
    "# here None means that a dimension can be of any length,\n",
    "# which is what we want, since the number of observations\n",
    "# we have can vary;\n",
    "# note that the shape argument to placeholder is optional, \n",
    "# but it allows TensorFlow to automatically catch bugs stemming \n",
    "# from inconsistent tensor shapes\n",
    "\n",
    "# x is placeholder for the 28X28 image data\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "#y_ is called \"y bar\" and is a 10 element vector,containing the predicted\n",
    "# probability of each digits(0-9) class. such as [0.14,0.8,0,0,0,0,0,0.06]\n",
    "y_ = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assigning bias and weights to null tensors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize both W and b as tensors full of zeros. \n",
    "# these are parameters that the model is later going to learn,\n",
    "# Notice that W has a shape of [784, 10] because we want to multiply \n",
    "# the 784-dimensional image vectors by it to produce 10-dimensional \n",
    "# vectors of evidence for the difference classes. b has a shape of [10] \n",
    "# so we can add it to the output.\n",
    "\n",
    "# Define weights and balance\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our model\n",
    "# to define the softmax classifier and cross entropy cost\n",
    "# we can do the following\n",
    "\n",
    "# matrix multiplication using the .matmul command\n",
    "# and add the softmax output\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is an activation function that is normally used in classification problems. It generate the probabilities for the output. For example, our model will not be 100% sure that one digit is the number nine, instead, the answer will be a distribution of probabilities where, if the model is right, the nine number will have a larger probability than the other other digits.\n",
    "\n",
    "Logistic function output is used for the classification between two target classes 0/1. Softmax function is generalized type of logistic function. That is, Softmax can output a multiclass categorical probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function: cross entropy, the reduce mean is simply the \n",
    " #average of the\n",
    "# cost function across all observation\n",
    "# Loss is cross entropy\n",
    "cross_entropy = tf.reduce_mean(\n",
    "             tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each training step in gradient decent we want to minimize cross entropy\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the global variable\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive session that can span multiple code blocks.Don't \n",
    "# forget to explicity close that session with sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the initialization which is only the initialization of all global \n",
    "# variables\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 1000 training steps\n",
    "for i in range(1000):\n",
    "    # get 100 random data points from the data batch_xs= images,\n",
    "    #batch_ys = digits(0-9) class\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100) \n",
    "    sess.run(train_step, feed_dict={x: batch_xs,y_: batch_ys}) # do the optimization with class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well the model did. Do this by comparing the digit with the\n",
    "# highest probability in actual(y) and predicted(y_)\n",
    "# here we're return the predicted class of each observation using argmax\n",
    "# and see if the ouput (prediction) is equal to the target variable (y)\n",
    "# since equal is a boolean type tensor, we cast it to a float type to compute\n",
    "# the actual accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9077"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not have to worry about computing the gradient to update the model, the nice thing about Tensorflow is that, once we've defined the structure of our model it has the capability to automatically differentiate mathematical expressions. This means we no longer need to compute the gradients ourselves! In this example, our softmax classifier obtained pretty nice result around 90%. But we can certainly do better with more advanced techniques such as convolutional deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref6\"></a>\n",
    "<h2>2nd Part: Deep Learning Applied on MNIST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish possible remaining session\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input object which reads data from mnist datasets.\n",
    "# perform one-hot encoding to define the digit\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initial Parameters <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Input and output<h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Create place holders for inputs and outputs<h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32, shape=[None, flat])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the mnist input data from list of values to a 28 pixel X 28 pixel \n",
    "# X 1 grayscale value cube which the convolution NN can use \n",
    "# Converting images of the data set to tensors\n",
    "x_image = tf.reshape(x,[-1,28,28,1],name='x_image')\n",
    "x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional Layer 1</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define helper functions to created weights and baises variables, and convolution, and pooling layer.\n",
    "we are using RELU as our activation function. These must be initialized to a small positive number and with some noise so you don't end up going to zero when comparing diffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional and pooling - we do convolution, and the pooling to control overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the max pooling<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],\n",
    "                    strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolutional Layer 1</h3>\n",
    "<h4>Weights and Biases of kernels</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Define layers in the NN <a>\n",
    "1st Convolution layer\n",
    "32 features for each 5X5 patch of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1= weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do convolution on images, add bias and push through RELU activation\n",
    "h_conv1= tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
    "\n",
    "# take result and run through max_pool\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the 32 features from convolution lay 1, in 5X5 patch.\n",
    "# Return 64 featurs weight and biases \n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do convolution of the output ofhte 1st convolution layer. pool results\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fully Connected Layer<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights and Biases between layer 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7*7*64,1024])\n",
    "b_fc1 = bias_variable([1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect output of pooling layer 2 as input to full connected lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dropout Layer, Optional phase for reducing overfitting<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "h_fc1_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Readout Layer (Softmax Layer)<h3>  \n",
    "   <h4>Weights and Biases<h4>\n",
    "<b>In last layer, CNN takes the high-level filtered images and translate them into votes using softmax. Input channels: 1024 (neurons from the 3rd Layer); 10 output features<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss measurement\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "logits=y_conv,labels=y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Predictions\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of steps and how often we display progress\n",
    "num_steps =2000\n",
    "display_every =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time =time.time()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    #Periodic status display\n",
    "    if i%display_every==0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        end_time = time.time()\n",
    "        print(\"step {0}, elapsed time {1:.2f} seconds , training accuracy {2:.2f}%\".format(i,end_time-start_time, float(train_accuracy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref9\"></a>\n",
    "<h2>Evaluate the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "     #  Time to train\n",
    "end_time = time.time()\n",
    "print(\"Total training time for {0} batches:{1:.2f} seconds\".format(i+1,end_time-start_time))\n",
    "\n",
    "# Accuracy on test data\n",
    "print(\"Total accuracy {0:.3f}%\".format(accuracy.eval(feed_dict={\n",
    "    x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})*100.0))\n",
    "              \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close() #finish the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Digital Recognition Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                   # package for computing\n",
    "from sklearn.model_selection import train_test_split  # split dataset\n",
    "import keras                # import keras with tensorflow as backend\n",
    "from keras.datasets import mnist         # import mnist dataset from keras \n",
    "from keras.models import Model, Sequential  # sequential and functional api keras \n",
    "from keras.layers import Dense,Conv2D,Dropout,Flatten,MaxPooling2D, Input   # dense and input layer for constructing mlp\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt     # matplotlib library for plotting\n",
    "\n",
    "# display plots inline (in notebook itself)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data\n",
    "# the data, split between train and validation set\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orginally shape (60000, 28, 28) for train and (10000, 28, 28) for test\n",
    "#but as we will be using fully connected layers we will flatten\n",
    "#the images into 1d array of 784 values instead of (28 x 28) 2d array\n",
    "train_x = train_x.reshape(train_x.shape[0],28,28,1)\n",
    "test_x = test_x.reshape(test_x.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As image is grayscale it has values from [0-255] which we will visualize below\n",
    "# convert dtype to float32 and scale the data from [0-255] to [0-1]\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize your training and test image datasets:\n",
    "train_x /=255\n",
    "test_x /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples and shape: 60000 (60000, 28, 28, 1)\n",
      "Test samples and shape: 10000 (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Training samples and shape:', train_x.shape[0], train_x.shape)\n",
    "print('Test samples and shape:', test_x.shape[0], test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation samples and shape 2000 (2000, 28, 28, 1)\n",
      "Test samples and shape 8000 (8000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# we will split val into --> 20% val set and 80% test set \n",
    "# stratify ensures the distribution of classes is same in both the sets\n",
    "\n",
    "val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.8, stratify=test_y)\n",
    "\n",
    "print ('Validation samples and shape', val_x.shape[0], val_x.shape)\n",
    "print ('Test samples and shape', test_x.shape[0], test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization data\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(2,20/2 , i +1,xticks=[],yticks=[])\n",
    "    ax.imshow(train_x[i].reshape(28,28),cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    ax.set_title(str(train_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0-9] unique labels\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "#train_y = keras.utils.to_categorical(train_y, num_classes)\n",
    "#val_y = keras.utils.to_categorical(val_y, num_classes)\n",
    "#test_y = keras.utils.to_categorical(test_y, num_classes)\n",
    "#print ('Training labels shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28,kernel_size=(3,3), input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.2136 - accuracy: 0.9367\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 19s 323us/step - loss: 0.0844 - accuracy: 0.9742\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 0.0588 - accuracy: 0.9821\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.0453 - accuracy: 0.9851\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 19s 322us/step - loss: 0.0341 - accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 19s 324us/step - loss: 0.0285 - accuracy: 0.9901\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 0.0238 - accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.0214 - accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.0188 - accuracy: 0.9934\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.0162 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13805f0d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy'\n",
    "             ,metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "model.fit(x=train_x, y= train_y,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06886547472439088, 0.984624981880188]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We achieved 98.5% accuracy with such basic model.\n",
    "# Many image classification cases(autonomous car),we can not even tolerate\n",
    "# 0.1% error since, as analogy it will cause 1 accident in 1000 cases.\n",
    "# However our model, I would say the result is still pretty good,\n",
    "# We can also make individual predictions with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-a8f528bb3897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4444\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_rows' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 4444\n",
    "plt.imshow(test_x[image_index].reshape(28,28), cmap='Greys')\n",
    "pred = model.predict(test_x[image_index].reshape(1, img_rows, img_cols, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to classify it as 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
